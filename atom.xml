<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Unsky Blog</title>
  <subtitle>Unsky Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-11-11T12:58:01.459Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>unsky</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>最大回文串 Longest Palindromic Substring</title>
    <link href="http://yoursite.com/2016/11/11/%E6%9C%80%E5%A4%A7%E5%9B%9E%E6%96%87%E4%B8%B2-Longest-Palindromic-Substring/"/>
    <id>http://yoursite.com/2016/11/11/最大回文串-Longest-Palindromic-Substring/</id>
    <published>2016-11-11T12:20:16.000Z</published>
    <updated>2016-11-11T12:58:01.459Z</updated>
    
    <content type="html"><![CDATA[<p>id5. Longest Palindromic Substring   QuestionEditorial Solution  My Submissions<br>Total Accepted: 147621<br>Total Submissions: 614546<br>Difficulty: Medium<br>Contributors: Admin<br>Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000.<br> <a id="more"></a></p>
<p>Example:<br>Input: “babad”</p>
<p>Output: “bab”</p>
<p>Note: “aba” is also a valid answer.<br>Example:</p>
<p>Input: “cbbd”</p>
<p>Output: “bb”<br>最长回文串,首先最容易想到的是穷举所有的字串，然后判断是不是回文。<br>最开始的方法使用递归判断是不是回文：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">//这是一种超时但是正确的做法。但是超时了。。</div><div class="line">#include&lt;iostream&gt;</div><div class="line">#include&lt;string&gt;</div><div class="line">using namespace std;</div><div class="line">bool isPalindromic(string s,int i,int j)</div><div class="line">&#123;</div><div class="line">    if (i==j)</div><div class="line">    return 1;</div><div class="line">    if(s[i]!=s[j])</div><div class="line">    return 0;</div><div class="line">    if (i+1==j)</div><div class="line">    return 1;</div><div class="line">    return isPalindromic(s,i+1,j-1);</div><div class="line">&#125;</div><div class="line">    string longestPalindrome(string s) &#123;</div><div class="line">        string longestPalindromeString =s.substr(0,1);</div><div class="line">        string str;</div><div class="line">        for(int i=0;i!=s.size();i++)</div><div class="line">        &#123;</div><div class="line">            for(int j=i;j!=s.size();j++)</div><div class="line">            &#123;</div><div class="line">                str=s.substr(i,j-i+1);</div><div class="line">             if(isPalindromic(str,0,str.size()-1)&amp;&amp;(str.size()&gt;longestPalindromeString.size()))</div><div class="line">             longestPalindromeString=str;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        return longestPalindromeString;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">int main()</div><div class="line">&#123; string s=&quot;sdgdfhfgjhgghkjhljkldyhrtiiiiytrewqtyitutyiyopuip&quot;;</div><div class="line">cout&lt;&lt;longestPalindrome(s);</div><div class="line">  return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>总结上面的做法，时间复杂度为  $o\left( n^2\log ^n \right) $<br>会发现做了很多重复的事情所以时间复杂度非常高，所以将思路变成穷举中心点，从中心按照奇数和偶数进行扩散，进而在扩散的过程就完成了回文的判断。节省了判断回文的时间。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">class Solution &#123;</div><div class="line">public:</div><div class="line">    string longestPalindrome(string s) &#123;</div><div class="line">        string longestPalindromeString =s.substr(0,1);</div><div class="line">        for(int i=0;i!=s.size();i++)</div><div class="line">        &#123;   //奇数</div><div class="line">            for(int j=0;((i-j)&gt;=0)&amp;&amp;((i+j)&lt;=s.size()-1);j++)</div><div class="line">            &#123;if(s[i-j]!=s[i+j])</div><div class="line">             break;</div><div class="line">             if ((2*j+1)&gt;longestPalindromeString.size())</div><div class="line">             longestPalindromeString=s.substr(i-j,2*j+1);</div><div class="line">             &#125;</div><div class="line">             //偶数</div><div class="line">            for(int j=0;((i-j)&gt;=0)&amp;&amp;((i+j+1)&lt;=s.size()-1);j++)</div><div class="line">            &#123; if(s[i-j]!=s[i+j+1])</div><div class="line">             break;</div><div class="line">             if ((2*j+2)&gt;longestPalindromeString.size())</div><div class="line">             longestPalindromeString=s.substr(i-j,2*j+2);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        return longestPalindromeString;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></p>
<p>这样的时间复杂度就为 $o\left( n^2 \right) $<br>顺别说一点，在写类的时候不要忘了在类后面加 <code>;</code>号，不然会提示</p>
<p><code>error: expected unqualified-id before string constant</code></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;id5. Longest Palindromic Substring   QuestionEditorial Solution  My Submissions&lt;br&gt;Total Accepted: 147621&lt;br&gt;Total Submissions: 614546&lt;br&gt;Difficulty: Medium&lt;br&gt;Contributors: Admin&lt;br&gt;Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000.&lt;br&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://yoursite.com/categories/leetcode/"/>
    
    
      <category term="最大回文串" scheme="http://yoursite.com/tags/%E6%9C%80%E5%A4%A7%E5%9B%9E%E6%96%87%E4%B8%B2/"/>
    
      <category term="Longest Palindromic Substring" scheme="http://yoursite.com/tags/Longest-Palindromic-Substring/"/>
    
  </entry>
  
  <entry>
    <title>激活函数</title>
    <link href="http://yoursite.com/2016/11/10/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2016/11/10/激活函数/</id>
    <published>2016-11-10T12:27:17.000Z</published>
    <updated>2016-11-11T14:06:45.873Z</updated>
    
    <content type="html"><![CDATA[<p>在神经元的数学模型中，轴突所携带的信号(例如: $x_0$ )通过突触进行传递，由于突触的强弱不一，假设我们以 $w_0$ 表示，那么我们传到下一个神经元的树突处的信号就变成了 $w_0x_0$ 。其中突触强弱(参数w)是可学的，它控制了一个神经元对另一个神经元影响的大小和方向（正负）。然后树突接收到信号后传递到神经元内部<code>cell body</code>，与其他树突传递过来的信号一起进行加和，如果这个和的值大于某一个固定的阈值的话，神经元就会被激活，然后传递冲激信号给树突。在数学模型中我们假设传递冲激信号的时间长短并不重要，只有神经元被激活的频率用于传递信息. 我们将是否激活神经元的函数称为激活函数(activation function $f$ ), 它代表了轴突接收到冲激信号的频率。以前我们比较常用的一个激活信号是<code>sigmoid function</code> $σ$ ，因为它接收一个实值的信号（即上面所说的加和的值）然后将它压缩到 <code>0-1</code> 的范围内。我们在后面会介绍更多的激活函数。<br> <a id="more"></a></p>
<h1 id="传统Sigmoid系激活函数"><a href="#传统Sigmoid系激活函数" class="headerlink" title="传统Sigmoid系激活函数"></a>传统Sigmoid系激活函数</h1><p><img src="/images/activate/3.png" alt=""><br>传统神经网络中最常用的两个激活函数，<code>Sigmoid</code>系（Logistic-Sigmoid、Tanh-Sigmoid）被视为神经网络的核心所在。从数学上来看，非线性的Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。</p>
<h2 id="sigmoid激活函数"><a href="#sigmoid激活函数" class="headerlink" title="sigmoid激活函数"></a>sigmoid激活函数</h2><p>sigmoid将一个实数输入映射到[0,1]范围内，如下图（左）所示。使用sigmoid作为激活函数存在以下几个问题：</p>
<ol>
<li>梯度饱和。当函数激活值接近于0或者1时，函数的梯度接近于0。在反向传播计算梯度过程中:<br>$$\delta^{\left(l\right)}=\left( W^{\left(l\right)} \right) ^T\delta ^{\left(l+1\right)}\cdot f’\left( z^{\left(L\right)}\right) $$<br>，每层残差接近于0，计算出的梯度也不可避免地接近于0。这样在参数微调过程中，会引起参数弥散问题，传到前几层的梯度已经非常靠近0了，参数几乎不会再更新。</li>
<li>函数输出不是以0为中心的。我们更偏向于当激活函数的输入是0时，输出也是0的函数.<h2 id="tanh激活函数"><a href="#tanh激活函数" class="headerlink" title="tanh激活函数"></a>tanh激活函数</h2></li>
</ol>
<p>tanh函数将一个实数输入映射到[-1,1]范围内，如上图（右）所示。当输入为0时，tanh函数输出为0，符合我们对激活函数的要求。然而，tanh函数也存在梯度饱和问题，导致训练效率低下。</p>
<h1 id="Softplus-amp-ReLu"><a href="#Softplus-amp-ReLu" class="headerlink" title="Softplus&amp;ReLu"></a>Softplus&amp;ReLu</h1><p><img src="/images/activate/4.png" alt=""></p>
<p>2001年，神经科学家Dayan、Abott从生物学角度，模拟出了脑神经元接受信号更精确的激活模型，该模型如左图所示：这个模型对比Sigmoid系主要变化有三点：<br><code>单侧抑制</code> <code>相对宽阔的兴奋边界</code> <code>稀疏激活性</code>（重点，可以看到红框里前端状态完全没有激活）<br>同年，Charles Dugas等人在做正数回归预测论文中偶然使用了Softplus函数，Softplus函数是Logistic-Sigmoid函数原函数。</p>
<h2 id="Softplus"><a href="#Softplus" class="headerlink" title="Softplus"></a>Softplus</h2><p>激活函数公式：<br>$$softplus\left( x \right) =\log \left( 1+e^x \right)<br>$$</p>
<p>按照论文的说法，一开始想要使用一个指数函数（天然正数）作为激活函数来回归，但是到后期梯度实在太大，难以训练，于是加了一个log来减缓上升趋势。<br>加了1是为了保证非负性。同年，Charles Dugas等人在NIPS会议论文中又调侃了一句，Softplus可以看作是强制非负校正函数 $max(0,x)$ 平滑版本。<br>偶然的是，同是2001年，ML领域的Softplus/Rectifier激活函数与神经科学领域的提出脑神经元激活频率函数有神似的地方，这促成了新的激活函数的研究。</p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>Relu激活函数（The Rectified Linear Unit）表达式为：<br>$$f(x)=max(0,x)$$</p>
<p><img src="/images/activate/5.png" alt=""></p>
<p>相比sigmoid和tanh函数，Relu激活函数的优点在于：</p>
<ol>
<li><code>梯度不饱和</code>。梯度计算公式为：1{x&gt;0}。因此在反向传播过程中，减轻了梯度弥散的问题，神经网络前几层的参数也可以很快的更新。</li>
<li><code>计算速度快</code>。正向传播过程中，sigmoid和tanh函数计算激活值时需要计算指数，而Relu函数仅需要设置阈值。如果x<0,f(x)=0，如果x>0,f(x)=x。加快了正向传播的计算速度。</0,f(x)=0，如果x></li>
</ol>
<p>因此，Relu激活函数可以极大地加快收敛速度，相比tanh函数，收敛速度可以加快6倍（如上图（右）所示）。</p>
<h1 id="激活函数的发展"><a href="#激活函数的发展" class="headerlink" title="激活函数的发展"></a>激活函数的发展</h1><h2 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h2><p>PReLU 是ReLU 和 LReLU的改进版本，具有非饱和性：<br>$$f\left( y_i \right) =\left{ \begin{array}{c}<br>    y_i\begin{matrix}<br>    &amp;        ,if\ y_i&gt;0\<br>\end{matrix}\<br>    a_iy_i\begin{matrix}<br>    &amp;        if\ y_i\le 0\<br>\end{matrix}\<br>\end{array} \right.<br>$$<br><img src="/images/activate/6.png" alt=""><br>与LReLU相比，PReLU中的负半轴斜率a可学习而非固定。原文献建议初始化a为0.25，不采用正则。个人认为，是否采用正则应当视具体的数据库和网络，通常情况下使用正则能够带来性能提升。</p>
<p>虽然PReLU 引入了额外的参数，但基本不需要担心过拟合。例如，在上述cifar10+NIN实验中， PReLU比ReLU和ELU多引入了参数，但也展现了更优秀的性能。所以实验中若发现网络性能不好，建议从其他角度寻找原因。</p>
<p>与ReLU相比，PReLU收敛速度更快。因为PReLU的输出更接近0均值，使得SGD更接近natural gradient。证明过程参见原文[10]。</p>
<p>此外，作者在ResNet 中采用ReLU，而没有采用新的PReLU。这里给出个人浅见，不一定正确，仅供参考。首先，在上述LReLU实验中，负半轴斜率对性能的影响表现出一致性。对PReLU采用正则将激活值推向0也能够带来性能提升。这或许表明，小尺度或稀疏激活值对深度网络的影响更大。其次，ResNet中包含单位变换和残差两个分支。残差分支用于学习对单位变换的扰动。如果单位变换是最优解，那么残差分支的扰动应该越小越好。这种假设下，小尺度或稀疏激活值对深度网络的影响更大。此时，ReLU或许是比PReLU更好的选择.</p>
<h2 id="RReLU"><a href="#RReLU" class="headerlink" title="RReLU"></a>RReLU</h2><p>数学形式与PReLU类似，但RReLU[9]是一种非确定性激活函数，其参数是随机的。这种随机性类似于一种噪声，能够在一定程度上起到正则效果。作者在cifar10/100上观察到了性能提升。</p>
<h2 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h2><p>Maxout[13]是ReLU的推广，其发生饱和是一个零测集事件（measure zero event）。正式定义为：<br>$$\max \left( w_{1}^{T}x+b_1,…,w_{n}^{T}x+b_n \right) $$<br>Maxout网络能够近似任意连续函数，且当w2,b2,…,wn,bn为0时，退化为ReLU。 其实，Maxout的思想在视觉领域存在已久。例如，在HOG特征里有这么一个过程：计算三个通道的梯度强度，然后在每一个像素位置上，仅取三个通道中梯度强度最大的数值，最终形成一个通道。这其实就是Maxout的一种特例。</p>
<p>Maxout能够缓解梯度消失，同时又规避了ReLU神经元死亡的缺点，但增加了参数和计算量。</p>
<h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p>ELU 融合了sigmoid和ReLU，具有左侧软饱性。其正式定义为：<br>$$f\left( x \right) =\left{ \begin{array}{c}<br>    x\begin{matrix}<br>    &amp;        ,if\ x&gt;0\<br>\end{matrix}\<br>    \alpha \left( \exp \left( x \right) -1 \right) \begin{matrix}<br>    &amp;        if\ x\le 0\<br>\end{matrix}\<br>\end{array} \right.$$<br><img src="/images/activate/7.png" alt=""><br>右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱能够让ELU对输入变化或噪声更鲁棒。ELU的输出均值接近于零，所以收敛速度更快。经本文作者实验，ELU的收敛性质的确优于ReLU和PReLU。在cifar10上，ELU 网络的loss 降低速度更快；在 ImageNet上，不加 Batch Normalization 30 层以上的 ReLU 网络会无法收敛，PReLU网络在MSRA的Fan-in （caffe ）初始化下会发散，而 ELU 网络在Fan-in/Fan-out下都能收敛 。</p>
<p>论文的另一个重要贡献是分析了Bias shift 现象与激活值的关系，证明了降低Bias shift 等价于把激活值的均值推向0。</p>
<h2 id="MPELU"><a href="#MPELU" class="headerlink" title="MPELU"></a>MPELU</h2><p>将分段线性与ELU统一到了一种形式下。在NIN+CIFAR10，本文作者发现ELU与LReLU性能一致，而与PReLU差距较大。经过分析，ELU泰勒展开的一次项就是LReLU。当在ELU前加入BN让输入集中在0均值附近， 则ELU与LReLU之差——泰勒展开高次项会变小，粗略估计，约55.57%的激活值误差小于0.01。因此，受PReLU启发，令α可学习能够提高性能。此外，引入参数β能够进一步控制ELU的函数形状。正式定义为<br>$$f\left( x \right) =\left{ \begin{array}{c}<br>    x\begin{matrix}<br>    &amp;        ,if\ x&gt;0\<br>\end{matrix}\<br>    \alpha \left( \exp \left( \beta x \right) -1 \right) \begin{matrix}<br>    &amp;        if\ x\le 0\<br>\end{matrix}\<br>\end{array} \right.$$<br><img src="/images/activate/8.png" alt=""><br>α 和 β可以使用正则。α, β 固定为1时，MPELU 退化为 ELU； β 固定为很小的值时，MPELU 近似为 PReLU；当α=0，MPELU 等价于 ReLU。</p>
<p>MPELU 的优势在于同时具备 ReLU、PReLU和 ELU的优点。首先，MPELU具备ELU的收敛性质，能够在无 Batch Normalization 的情况下让几十层网络收敛。其次，作为一般化形式， MPELU较三者的推广能力更强。简言之，MPELU = max(ReLU, PReLU, ELU)。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在神经元的数学模型中，轴突所携带的信号(例如: $x_0$ )通过突触进行传递，由于突触的强弱不一，假设我们以 $w_0$ 表示，那么我们传到下一个神经元的树突处的信号就变成了 $w_0x_0$ 。其中突触强弱(参数w)是可学的，它控制了一个神经元对另一个神经元影响的大小和方向（正负）。然后树突接收到信号后传递到神经元内部&lt;code&gt;cell body&lt;/code&gt;，与其他树突传递过来的信号一起进行加和，如果这个和的值大于某一个固定的阈值的话，神经元就会被激活，然后传递冲激信号给树突。在数学模型中我们假设传递冲激信号的时间长短并不重要，只有神经元被激活的频率用于传递信息. 我们将是否激活神经元的函数称为激活函数(activation function $f$ ), 它代表了轴突接收到冲激信号的频率。以前我们比较常用的一个激活信号是&lt;code&gt;sigmoid function&lt;/code&gt; $σ$ ，因为它接收一个实值的信号（即上面所说的加和的值）然后将它压缩到 &lt;code&gt;0-1&lt;/code&gt; 的范围内。我们在后面会介绍更多的激活函数。&lt;br&gt;
    
    </summary>
    
      <category term="卷积深度网络全剖析" scheme="http://yoursite.com/categories/%E5%8D%B7%E7%A7%AF%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E5%85%A8%E5%89%96%E6%9E%90/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="激活函数" scheme="http://yoursite.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>损失函数</title>
    <link href="http://yoursite.com/2016/11/10/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2016/11/10/损失函数/</id>
    <published>2016-11-10T09:44:34.000Z</published>
    <updated>2016-11-10T12:22:41.222Z</updated>
    
    <content type="html"><![CDATA[<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;损失函数&quot;&gt;&lt;a href=&quot;#损失函数&quot; class=&quot;headerlink&quot; title=&quot;损失函数&quot;&gt;&lt;/a&gt;损失函数&lt;/h1&gt;
    
    </summary>
    
      <category term="卷积深度网络全剖析" scheme="http://yoursite.com/categories/%E5%8D%B7%E7%A7%AF%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E5%85%A8%E5%89%96%E6%9E%90/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="损失函数" scheme="http://yoursite.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>聚类分析</title>
    <link href="http://yoursite.com/2016/11/09/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2016/11/09/聚类分析/</id>
    <published>2016-11-09T09:33:59.000Z</published>
    <updated>2016-11-10T12:24:59.563Z</updated>
    
    <content type="html"><![CDATA[<p>聚类分析将数据划分成有意义或有用的组（簇）。聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据对象分组。其目标是，组内的对象相互之间是相似的，而不同组中的对象是不同的。<br> <a id="more"></a></p>
<h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>一个好的聚类方法要能产生高质量的聚类结果——簇，这些簇要具备以下两个特点：</p>
<ol>
<li>高的簇内相似性</li>
<li>低的簇间相似性</li>
</ol>
<p>聚类结果的好坏取决于该聚类方法采用的相似性评估方法以及该方法的具体实现<br>聚类方法的好坏还取决于该方法是否能发现某些还是所有的隐含模式</p>
<p><img src="/images/clutster/1.png" alt=""></p>
<p>聚类可以分为：</p>
<ol>
<li>划分聚类（Partitional Clustering）</li>
<li>层次聚类（Hierarchical Clustering）</li>
<li>互斥（重叠）聚类（exclusive clustering）</li>
<li>非互斥聚类（non-exclusive）</li>
<li>模糊聚类（fuzzy clustering）</li>
<li>完全聚类（complete clustering）</li>
<li>部分聚类（partial clustering）</li>
</ol>
<h2 id="划分聚类"><a href="#划分聚类" class="headerlink" title="划分聚类"></a>划分聚类</h2><p>划分聚类简单地将数据对象集划分成不重叠的子集，使得每个数据对象恰在一个子集。</p>
<p><img src="/images/clutster/2.png" alt=""></p>
<h2 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h2><p>层次聚类是嵌套簇的集族，组织成一棵树。<br><img src="/images/clutster/3.png" alt=""></p>
<h2 id="互斥聚类（exclusive）"><a href="#互斥聚类（exclusive）" class="headerlink" title="互斥聚类（exclusive）"></a>互斥聚类（exclusive）</h2><p>每个对象都指派到单个簇.</p>
<h2 id="重叠聚类（overlapping）或非互斥聚类（non-exclusive）"><a href="#重叠聚类（overlapping）或非互斥聚类（non-exclusive）" class="headerlink" title="重叠聚类（overlapping）或非互斥聚类（non-exclusive）"></a>重叠聚类（overlapping）或非互斥聚类（non-exclusive）</h2><p>聚类用来反映一个对象.同时属于多个组（类）这一事实。<br>例如：在大学里，一个人可能既是学生，又是雇员</p>
<h2 id="模糊聚类（Fuzzy-clustering-）"><a href="#模糊聚类（Fuzzy-clustering-）" class="headerlink" title="模糊聚类（Fuzzy clustering ）"></a>模糊聚类（Fuzzy clustering ）</h2><p>每个对象以一个0（绝对不属于）和1（绝对属于）之间的隶属权值属于每个簇。<br>换言之，簇被视为模糊集。</p>
<h2 id="部分聚类（Partial）"><a href="#部分聚类（Partial）" class="headerlink" title="部分聚类（Partial）"></a>部分聚类（Partial）</h2><p>部分聚类中数据集某些对象可能不属于明确定义的组。如：一些对象可能是离群点、噪声。</p>
<h2 id="完全聚类（complete）"><a href="#完全聚类（complete）" class="headerlink" title="完全聚类（complete）"></a>完全聚类（complete）</h2><p>完全聚类将每个对象指派到一个簇。</p>
<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><h2 id="簇的分类"><a href="#簇的分类" class="headerlink" title="簇的分类"></a>簇的分类</h2><p>簇是同一性状物体的集合<br>按照性状的分类不同可以将簇分为</p>
<ol>
<li>明显分离的</li>
</ol>
<p>每个点到同簇中任一点的距离比到不同簇中所有点的距离更近。</p>
<ol>
<li>基于原型的</li>
</ol>
<p>每个对象到定义该簇的原型的距离比到其他簇的原型的距离更近。<br>对于具有连续属性的数据，簇的原型通常是质心，即簇中所有点的平均值。<br>当质心没有意义时，原型通常是中心点，即簇中最有代表性的点。</p>
<p>基于中心的（ Center-Based）的簇：<br>每个点到其簇中心的距离比到任何其他簇中心的距离更近。</p>
<ol>
<li>基于图的</li>
</ol>
<p>如果数据用图表示，其中节点是对象，而边代表对象之间的联系。</p>
<p>簇可以定义为连通分支（connected component）：互相连通但不与组外对象连通的对象组。</p>
<p>基于近邻的（ Contiguity-Based）：其中两个对象是相连的，仅当它们的距离在指定的范围内。这意味着，每个对象到该簇某个对象的距离比到不同簇中任意点的距离更近。</p>
<ol>
<li>基于密度的</li>
</ol>
<p>簇是对象的稠密区域，被低密度的区域环绕。</p>
<ol>
<li><p>概念簇<br>可以把簇定义为有某种共同性质的对象的集合。<br>例如：基于中心的聚类。还有一些簇的共同性质需要更复杂的算法才能识别出来。</p>
<p>本系列文章将探究各个聚类方式之间的区别</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;聚类分析将数据划分成有意义或有用的组（簇）。聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据对象分组。其目标是，组内的对象相互之间是相似的，而不同组中的对象是不同的。&lt;br&gt;
    
    </summary>
    
      <category term="聚类分析" scheme="http://yoursite.com/categories/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类分析" scheme="http://yoursite.com/tags/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>互斥聚类-k-means聚类</title>
    <link href="http://yoursite.com/2016/11/09/k-means%E8%81%9A%E7%B1%BB/"/>
    <id>http://yoursite.com/2016/11/09/k-means聚类/</id>
    <published>2016-11-09T08:56:52.000Z</published>
    <updated>2016-11-10T12:11:58.912Z</updated>
    
    <content type="html"><![CDATA[<p> 在数据挖掘中，K-Means算法是一种cluster analysis的算法， 其主要是来计算数据聚集算法，主要通过不断地取离种子点最近均值的算法。<br> <a id="more"></a></p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>K-Means算法主要解决的问题如下图所示。我们可以看到，在图的左边有一些点，我们用肉眼可以看出来有四个点群，但是我们怎么通过计算机程序找出这几个点群来呢？</p>
<p>于是就出现了我们的K-Means算法。<br><img src="/images/kmeans/1.png" alt=""><br>算法概要如下图所示：<br><img src="/images/kmeans/2.png" alt=""><br>从上图中，我们可以看到，A，B，C，D，E是五个在图中点。而灰色的点是我们的种子点，也就是我们用来找点群的点。有两个种子点，所以K=2。</p>
<p>然后，K-Means的算法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">1. 随机在图中取K（这里K=2）个种子点</div><div class="line"></div><div class="line">2. 然后对图中的所有点求到这K个种子点的距离，假如点Pi离种子点Si最近，</div><div class="line">那么Pi属于Si点群。</div><div class="line">（上图中，我们可以看到A，B属于上面的种子点，C，D，E属于下面中部的种子点）</div><div class="line">3. 我们要移动种子点到属于他的“点群”的中心。（见图上的第三步）</div><div class="line">然后重复第2）和第3）步直到，种子点没有移动（我们可以看到图中的第四步上面的种子点聚合了A，B，C，下面的种子点聚合了D，E）。</div></pre></td></tr></table></figure>
<p>使用公式可以表示为：<br>$$\varUpsilon =\sum_{k=1}^K{\sum_{i=1}^L{dist\left( x_i-u_k \right)}}$$<br>而$dist$距离函数可以分为：</p>
<ol>
<li>Minkowski Distance（闵可夫斯基距离）<br>$$<br>d_{i,j}=\sqrt[\lambda]{\sum_{k=1}^n{|x_{i,k}-x_{j,k}|}^{\lambda}}<br>$$<br>$\lambda$ 可以随意取值，可以是负数，也可以是正数，或是无穷大</li>
<li>Euclidean Distance(欧拉距离)</li>
</ol>
<p>$$<br>d_{i,j}=\sqrt[]{\sum_{k=1}^n{|x_{i,k}-x_{j,k}|}^2}<br>$$<br>其形式为 Minkowski Distance 为 $\lambda$=2时候的特殊形式。</p>
<ol>
<li>CityBlock Distance （CB距离）<br>$$<br>d_{i,j}=\sum_{k=1}^n{|x_{i,k}-x_{j,k}|}<br>$$<br>也就是第一个公式 $\lambda$=1的情况。</li>
</ol>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><p>在模拟k-means的实验中我们使用深度学习网络vgg16在一个层的feature maps 来进行模拟实验，用来验证聚类效果，输入图片为一个行人。我们使用vgg16 conv1-2产生的feature maps进行实验。假设我们产生的feature maps 为 $\psi \in R^{m\times n\times k}$ ,其中 $m\times n$ 为feature大小, k为map的个数。首先每个feature进行向量化，从而将其转化为 $\psi \in R^{m* n\times k}$ 。使用k-means对一个层的feature maps进行聚类，并可视化效果。本实验中使用欧式距离。</p>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><ol>
<li>实验使用vlfeat工具包<br>下载地址： <a href="https://github.com/unsky/vlfeat" target="_blank" rel="external">https://github.com/unsky/vlfeat</a></li>
<li>在vlfeat\toolbox中使用vl_setup启动工具包</li>
</ol>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><p>假设feature maps已经存在于 res1.mat 中<br>运行代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">load(&apos;res1.mat&apos;);</div><div class="line">a=res(23).x;</div><div class="line">[w,e,r]=size(a);</div><div class="line">a=reshape(a,w*e,r);</div><div class="line"> [C, A] = vl_kmeans(a, 10) ;</div><div class="line"> for i=1:10,</div><div class="line"> fea=C(:,i);</div><div class="line"> fea=reshape(fea,w,e);</div><div class="line">  n = mapminmax(fea, 0, 1);</div><div class="line"> subplot(2,5,i);</div><div class="line"> imshow(n);</div><div class="line"> hold on;</div><div class="line"> end</div></pre></td></tr></table></figure></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><ol>
<li>原始64个feature的可视化：<br><img src="/images/kmeans/3.png" alt=""></li>
<li>k-means （k=10）<br><img src="/images/kmeans/4.png" alt=""></li>
</ol>
<p>k=5<br><img src="/images/kmeans/5.png" alt=""></p>
<p>k=1<br><img src="/images/kmeans/6.png" alt=""></p>
<h1 id="k-means-优缺点"><a href="#k-means-优缺点" class="headerlink" title="k-means 优缺点"></a>k-means 优缺点</h1><p> 优点：</p>
<ol>
<li>算法简单</li>
<li>适用于球形簇</li>
<li>二分k均值等变种算法运行良好，不受初始化问题的影响。</li>
</ol>
<p>缺点：</p>
<ol>
<li>不能处理非球形簇、不同尺寸和不同密度的簇</li>
<li>对离群点、噪声敏感</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 在数据挖掘中，K-Means算法是一种cluster analysis的算法， 其主要是来计算数据聚集算法，主要通过不断地取离种子点最近均值的算法。&lt;br&gt;
    
    </summary>
    
      <category term="聚类分析" scheme="http://yoursite.com/categories/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="k-means" scheme="http://yoursite.com/tags/k-means/"/>
    
      <category term="聚类" scheme="http://yoursite.com/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>手把手将atom打造成c++/c编程利器</title>
    <link href="http://yoursite.com/2016/11/08/%E6%89%8B%E6%8A%8A%E6%89%8B%E5%B0%86atom%E6%89%93%E9%80%A0%E6%88%90c-c%E7%BC%96%E7%A8%8B%E5%88%A9%E5%99%A8/"/>
    <id>http://yoursite.com/2016/11/08/手把手将atom打造成c-c编程利器/</id>
    <published>2016-11-08T02:31:28.000Z</published>
    <updated>2016-11-10T12:11:56.449Z</updated>
    
    <content type="html"><![CDATA[<p>Atom是由GitHub开发的自由及开放源代码的文字与代码编辑器，支持OS X、Windows和Linux操作系统，支持Node.js所写的插件，并内置Git版本控制系统。多数的延伸包皆为开放源代码授权，并由社区建置与维护。Atm基于Chromium并使用CoffeeScript撰写。Atom也可当作IDE使用。<br> <a id="more"></a></p>
<h1 id="安装atom"><a href="#安装atom" class="headerlink" title="安装atom"></a>安装atom</h1><p> 原则自己的平台安装，本文所有操作均在 win10中进行。</p>
<p> 官方下载地址：<a href="https://atom.io/" target="_blank" rel="external">https://atom.io/</a></p>
<p> 安装之后进入欢迎界面<br> <img src="/images/atomle/1.png" alt=""><br>原始的atom已经安装了一些插件。但是我们需要将其打造成一个c/c++ IDE.</p>
<h1 id="编译和调试插件-gpp-compliler"><a href="#编译和调试插件-gpp-compliler" class="headerlink" title="编译和调试插件 gpp-compliler"></a>编译和调试插件 gpp-compliler</h1><p>编译和调试是ide的基本功能不可或缺。</p>
<p>gpp-compliler 安装依赖于 MinGW 下载地址：<a href="http://www.mingw.org/" target="_blank" rel="external">http://www.mingw.org/</a></p>
<p>MinGW(Minimalist GNU For Windows)是个精简的Windows平台C/C++、ADA及Fortran编译器，相比Cygwin而言，体积要小很多，使用较为方便。</p>
<h2 id="运行刚刚下载的安装程序"><a href="#运行刚刚下载的安装程序" class="headerlink" title="运行刚刚下载的安装程序"></a>运行刚刚下载的安装程序</h2><p> <img src="/images/atomle/2.png" alt=""><br> 安装到D:\MinGW，点”Continue”。之后在桌面会形成一个安装器：<br>  <img src="/images/atomle/3.png" alt=""><br>  运行</p>
<h2 id="选择安装组件"><a href="#选择安装组件" class="headerlink" title="选择安装组件"></a>选择安装组件</h2><p>  运行安装程序。<img src="/images/atomle/4.png" alt="">  选择需要安装的组件，右键选择Mark for Installation,之后选择Installation -&gt; Apply Changes。</p>
<h2 id="GCC"><a href="#GCC" class="headerlink" title="GCC"></a>GCC</h2><p>  这里重点要提到的是GCC组件的安装，如图所示<br>      <img src="/images/atomle/5.png" alt=""><br>      先选择左边的”MinGW Base System”选项，之后再右边找到mingw-gcc。最好选择bin,dev和lic三个组件进行安装。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p> 设置MinGW环境变量<br>鼠标右键“我的电脑”-&gt;“属性”，选择“高级”选项卡下的“环境变量”，在系统变量里点“新建”，之后填写MinGW的安装路径，如下：<br>      <img src="/images/atomle/6.png" alt=""><br>      之后找到Path，在最前面添加下面这段声明，之后点击确定。安装完成后，在MinGW\bin的目录下，会有一个名为gcc.exe的可执行文件。在 <code>cmd</code> 运行  <code>gcc -v</code>会出现版本信息，如果显示，则安装成功。回到atom,新建一个cpp文件，代码中我们加个错误</p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#include &lt;iostream&gt;</div><div class="line">#include &lt;vector&gt;</div><div class="line">using namespace std;</div><div class="line">int main()&#123;</div><div class="line">    int aaa;</div><div class="line">    cin&gt;&gt;aaa;</div><div class="line">aa</div><div class="line">  cout&lt;&lt;aaa&lt;&lt;endl;</div><div class="line"></div><div class="line">        return 0;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</code></pre><p>  运行 <code>F5</code><br>    <img src="/images/atomle/7.png" alt=""><br>    错误提示在右上角。<br>    将代码修改正确<br>        <img src="/images/atomle/8.png" alt=""><br>        显示控制台结果.</p>
<h1 id="代码错误检测-linter-gcc"><a href="#代码错误检测-linter-gcc" class="headerlink" title="代码错误检测 linter-gcc"></a>代码错误检测 linter-gcc</h1><p>linter-gcc 安装依赖于linter 首先安装linter包。同时还依赖于 gcc 在上一步我们已经安装了gcc并且已经加入了路径，如果没有请自行安装。</p>
<p>只需在 linter-gcc的设置里面<br> <img src="/images/atomle/9.png" alt=""><br><code>F5</code> 运行一段代码错误代码，可以显示：<br><img src="/images/atomle/10.png" alt=""><br>  到这里 atom已经可以完成我们编译所需的环境，剩下的可以进一步就行优化推荐一些优秀的插件</p>
<h1 id="高亮选择-highlight-selected"><a href="#高亮选择-highlight-selected" class="headerlink" title="高亮选择 highlight-selected"></a>高亮选择 highlight-selected</h1><p>可以高亮选择的关键字<br>    <img src="/images/atomle/11.png" alt=""></p>
<h1 id="自动美化代码-atom-beautiful"><a href="#自动美化代码-atom-beautiful" class="headerlink" title="自动美化代码 atom-beautiful"></a>自动美化代码 atom-beautiful</h1><h1 id="minimap"><a href="#minimap" class="headerlink" title="minimap"></a>minimap</h1><p>和submit一种风格的快速浏览窗口。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Atom是由GitHub开发的自由及开放源代码的文字与代码编辑器，支持OS X、Windows和Linux操作系统，支持Node.js所写的插件，并内置Git版本控制系统。多数的延伸包皆为开放源代码授权，并由社区建置与维护。Atm基于Chromium并使用CoffeeScript撰写。Atom也可当作IDE使用。&lt;br&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="atom" scheme="http://yoursite.com/tags/atom/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络</title>
    <link href="http://yoursite.com/2016/11/02/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E5%8E%86%E7%A8%8B/"/>
    <id>http://yoursite.com/2016/11/02/卷积神经网络学习历程/</id>
    <published>2016-11-01T16:10:30.000Z</published>
    <updated>2016-11-10T12:22:23.958Z</updated>
    
    <content type="html"><![CDATA[<p>  神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好效果。然而，讨论将还是从对生物系统的一个高层次的简略描述开始，因为神经网络毕竟是从这里得到了启发.<br><a id="more"></a></p>
<h1 id="神经元建模"><a href="#神经元建模" class="headerlink" title="神经元建模"></a>神经元建模</h1><p>  大脑的基本计算单位是神经元（neuron）。人类的神经系统中大约有860亿个神经元，它们被大约10^14-10^15个突触（synapses）连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的树突获得输入信号，然后沿着它唯一的轴突（axon）产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。</p>
<p>  在神经元的计算模型中，沿着轴突传播的信号（比如 $x_0$ ）将基于突触的突触强（比如 $w_0$ ），与其他神经元的树突进行乘法交互（比如$w_0x_0$）。其观点是，突触的强度（也就是权重），是可学习的且可以控制一个神经元对于另一个神经元的影响强度（还可以控制影响方向：使其兴奋（正权重）或使其抑制（负权重））。在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。如果最终之和高于某个阈值，那么神经元将会激活，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号的频率在交流信息。基于这个速率编码的观点，将神经元的激活率建模为激活函数（activation function）f，它表达了轴突上激活信号的频率。由于历史原因，激活函数常常选择使用sigmoid函数 $\sigma$，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。在本节后面部分会看到这些激活函数的各种细节。</p>
<p>  <img src="/images/cnn/1.png" alt=""><br>  左边是生物神经元，右边是数学模型。</p>
<h1 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h1><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>自然图像有其固有特性，也就是说，图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。</p>
<p>更恰当的解释是，当从一个大尺寸图像中随机选取一小块，比如说 8x8 作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个 8x8 样本中学习到的特征作为探测器，应用到这个图像的任意地方中去。特别是，我们可以用从 8x8 样本中所学习到的特征跟原本的大尺寸图像作卷积，从而对这个大尺寸图像上的任一位置获得一个不同特征的激活值。</p>
<p>下面给出一个具体的例子：假设你已经从一个 96x96 的图像中学习到了它的一个 8x8 的样本所具有的特征，假设这是由有 100 个隐含单元的自编码完成的。为了得到卷积特征，需要对 96x96 的图像的每个 8x8 的小块图像区域都进行卷积运算。也就是说，抽取8x8 的小块区域，并且从起始坐标开始依次标记为（1，1），（1，2），…，一直到（89，89），然后对抽取的区域逐个运行训练过的稀疏自编码来得到特征的激活值。在这个例子里，显然可以得到 100 个集合，每个集合含有 89x89 个卷积特征。<br>  <img src="/images/cnn/2.gif" alt=""></p>
<p>  假设给定了 $r \times c$ 的大尺寸图像，将其定义为 xlarge。首先通过从大尺寸图像中抽取的 $a \times b$ 的小尺寸图像样本 $x_{small}$ 训练稀疏自编码，计算 $f = σ(W(1)x_{small} + b(1))$ （σ 是一个 sigmoid 型函数）得到了 k 个特征， 其中 W(1) 和 b(1) 是可视层单元和隐含单元之间的权重和偏差值。对于每一个 $a \times b$ 大小的小图像 $x_s$，计算出对应的值  $f_s = σ(W(1)x_s + b(1))$ ，对这些 fconvolved 值做卷积，就可以得到  $k \times (r - a + 1) \times (c - b + 1)$  个卷积后的特征的矩阵。</p>
<h3 id="窄卷积-vs-宽卷积"><a href="#窄卷积-vs-宽卷积" class="headerlink" title="窄卷积 vs 宽卷积"></a>窄卷积 vs 宽卷积</h3><p>在上文中解释卷积运算的时候，忽略了如何使用滤波器的一个小细节。在矩阵的中部使用3x3的滤波器没有问题，在矩阵的边缘该怎么办呢？左上角的元素没有顶部和左侧相邻的元素，该如何滤波呢？解决的办法是采用补零法（zero-padding）。所有落在矩阵范围之外的元素值都默认为0。这样就可以对输入矩阵的每一个元素做滤波了，输出一个同样大小或是更大的矩阵。补零法又被称为是宽卷积，不使用补零的方法则被称为窄卷积。在具体的试验中就是<code>pad</code>字段设置 <code>0 or other</code></p>
<p>1D的例子如图所示：<br>  <img src="/images/cnn/3.png" alt="">窄卷积 vs 宽卷积。滤波器长度为5，输入长度为7。</p>
<h3 id="步长"><a href="#步长" class="headerlink" title="步长"></a>步长</h3><p>卷积运算的另一个超参数是步长，即每一次滤波器平移的距离。上面所有例子中的步长都是1，相邻两个滤波器有重叠。步长越大，则用到的滤波器越少，输出的值也越少.<br>在实验中使用<code>strike</code>进行控制<br>  <img src="/images/cnn/4.png" alt=""></p>
<h2 id="池化-pooling"><a href="#池化-pooling" class="headerlink" title="池化 pooling"></a>池化 pooling</h2><p>卷积神经网络的一个重要概念就是池化层，一般是在卷积层之后。池化层对输入做降采样。常用的池化做法是对每个滤波器的输出求最大值。我们并不需要对整个矩阵都做池化，可以只对某个窗口区间做池化。例如，下图所示的是2x2窗口的最大值池化（在NLP里，我们通常对整个输出做池化，每个滤波器只有一个输出值）：<br>  <img src="/images/cnn/5.png" alt=""></p>
<p>池化的特点之一就是它输出一个固定大小的矩阵，这对分类问题很有必要。例如，如果你用了1000个滤波器，并对每个输出使用最大池化，那么无论滤波器的尺寸是多大，也无论输入数据的维度如何变化，你都将得到一个1000维的输出。这让你可以应用不同长度的句子和不同大小的滤波器，但总是得到一个相同维度的输出结果，传入下一层的分类器。</p>
<p>池化还能降低输出结果的维度，（理想情况下）却能保留显著的特征。你可以认为每个滤波器都是检测一种特定的特征，例如，检测句子是否包含诸如“not amazing”等否定意思。如果这个短语在句子中的某个位置出现，那么对应位置的滤波器的输出值将会非常大，而在其它位置的输出值非常小。通过采用取最大值的方式，能将某个特征是否出现在句子中的信息保留下来，但是无法确定它究竟在句子的哪个位置出现。这个信息出现的位置真的很重要吗？确实是的，它有点类似于一组n-grams模型的行为。尽管丢失了关于位置的全局信息（在句子中的大致位置），但是滤波器捕捉到的局部信息却被保留下来了，比如“not amazing”和“amazing not”的意思就大相径庭。</p>
<p>在图像识别领域，池化还能提供平移和旋转不变性。若对某个区域做了池化，即使图像平移/旋转几个像素，得到的输出值也基本一样，因为每次最大值运算得到的结果总是一样的</p>
<h2 id="前向"><a href="#前向" class="headerlink" title="前向"></a>前向</h2><p>现在设节点 $i$ 和节点 $j$ 之间的权值为 $w_{i,j}$ ，节点 $j$ 的阀值为 $b_j$ ，每个节点的输出值为 $x_j$ ，而每个节点的输出值是根据上层所有节点的输出值、当前节点与上一层所有节点的权值和当前节点的阀值还有激活函数来实现的。具体计算方法如下</p>
<p>$$S_j=\sum_{i=0}^{m-1}{w_{i,j}x_i+b_j}$$<br>$$x_j=f\left( S_j \right) $$</p>
<p>其中 $f$ 为激活函数，一般选取S型函数或者线性函数。正向传递的过程比较简单，按照上述公式计算即可。在BP神经网络中，输入层节点没有阀值。</p>
<h3 id="卷积的前向"><a href="#卷积的前向" class="headerlink" title="卷积的前向"></a>卷积的前向</h3><p>如下图，卷积层的输入来源于输入层或者pooling层。每一层的多个卷积核大小相同，在这个网络中，使用的卷积核均为 $5\times 5$ 。<br>  <img src="/images/cnn/6.png" alt=""></p>
<p>如图输入为 $28 \times 28$ 的图像，经过 $5 \times5$ 的卷积之后，得到一个 $(28-5+1)\times (28-5+1) = 24\times 24$ 的map。卷积层2的每个map是不同卷积核在前一层每个map上进行卷积，并将每个对应位置上的值相加然后再加上一个偏置项。</p>
<p>  <img src="/images/cnn/7.png" alt=""><br>每次用卷积核与map中对应元素相乘，然后移动卷积核进行下一个神经元的计算。如图中矩阵C的第一行第一列的元素2，就是卷积核在输入map左上角时的计算结果。在图中也很容易看到，输入为一个 $4\times 4$ 的map，经过 $2\times 2$ 的卷积核卷积之后，结果为一个 $(4-2+1) \times (4-2+1) = 3\times 3$ 的map。</p>
<h3 id="卷积前向的caffe实现"><a href="#卷积前向的caffe实现" class="headerlink" title="卷积前向的caffe实现"></a>卷积前向的caffe实现</h3><p>Caffe中卷积的实现十分巧妙，详细可以参考一下这篇论文: <a href="https://hal.archives-ouvertes.fr/file/index/docid/112631/filename/p1038112283956.pdf" target="_blank" rel="external">https://hal.archives-ouvertes.fr/file/index/docid/112631/filename/p1038112283956.pdf</a></p>
<p>下面是一张论文中的图片，看这张图片可以很清楚理解。从图中可以看出，卷积之前将输入的多个矩阵和多个卷积核先展开再组合成2个大的矩阵，用展开后的矩阵相乘。</p>
<p><img src="/images/cnn/8.png" alt=""></p>
<p>假设我们一次训练16张图片(即batch_size为16)。通过之前的推导，我们知道该层的输入为20个 $12\times 12$ 的特征图，所以bottom的维度 $16\times 20\times 12\times 12$ ，则该层的输出top的维度为 $16\times 50 \times 8\times 8$ 。</p>
<h1 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h1><h2 id="反向梯度"><a href="#反向梯度" class="headerlink" title="反向梯度"></a>反向梯度</h2><p>在BP神经网络中，误差信号反向传递子过程比较复杂，它是基于 <code>Widrow-Hoff</code> 学习规则的。假设输出层的所有结果为，误差函数如下<br>$$E\left( w,b \right) =\frac{1}{2}\sum_{j=0}^{n-1}{\left( d_j-y_j \right) ^2}$$</p>
<p>而BP神经网络的主要目的是反复修正权值和阀值，使得误差函数值达到最小。<code>Widrow-Hoff</code> 学习规则是通过沿着相对误差平方和的最速下降方向，连续调整网络的权值和阀值，根据梯度下降法，权值矢量的修正正比于当前位置上 $E(w,b)$ 的梯度，对于第 $j$ 个输出节点有<br>$$\varDelta w\left( i,j \right) =-\eta \frac{\partial E\left( w,b \right)}{\partial w\left( i,j \right)}$$<br>假设我们选择激活函数：<br>$$f\left( x \right) =\frac{A}{1+e^{-\frac{x}{B}}}$$<br>对其进行求导：<br>$$<br>f’\left( x \right) =\frac{Ae^{-\frac{x}{B}}}{B\left( 1+e^{-\frac{x}{B}} \right) ^2}<br>$$<br>$$<br>=\frac{1}{AB}\cdot \frac{A}{1+e^{-\frac{x}{B}}}\cdot \left( A-\frac{A}{1+e^{-\frac{x}{B}}} \right)<br>$$<br>$$<br>=\frac{f\left( x \right) \left[ A-f\left( x \right) \right]}{AB}<br>$$<br>那么接下来针对 $w_{i,j}$</p>
<p>$$ \frac{\partial E\left( w,b \right)}{\partial w_{ij}}=\frac{1}{\partial w_{ij}}\cdot \frac{1}{2}\sum_{j=0}^{n-1}{\left( d_j-y_j \right) ^2} $$</p>
<p>$$<br>=\left( d_j-y_j \right) \cdot \frac{\partial d_j}{\partial w_{ij}}<br>$$</p>
<p>$$<br>=\left( d_j-y_j \right) \cdot f’\left( S_j \right) \frac{\partial S_j}{\partial w_{ij}}<br>$$</p>
<p>$$<br>=\left( d_j-y_j \right) \frac{f\left( S_j \right) \left[ A-f\left( S_j \right) \right]}{AB}<br>$$</p>
<p>$$<br>=\left( d_j-y_j \right) \frac{f\left( S_j \right) \left[ A-f\left( S_j \right) \right]}{AB}\cdot x_i<br>$$</p>
<p>$$=\delta_{ij}\cdot x_j$$</p>
<p>其中有</p>
<p>$$\delta_{ij}=\left( d_j-y_j \right) \frac{f\left( S_j \right) \left[ A-f\left( S_j \right) \right]}{AB} $$</p>
<p>同样，对于 $b_j$<br>$$<br>\frac{\partial E\left( w,b \right)}{\partial b_j}=\delta_{ij}<br>$$</p>
<p>这就是著名的 $\delta$   学习规则，通过改变神经元之间的连接权值来减少系统实际输出和期望输出的误差，这个规则又叫做<code>Widrow-Hoff</code>学习规则或者纠错学习规则。</p>
<p>上面是对隐含层和输出层之间的权值和输出层的阀值计算调整量，而针对输入层和隐含层和隐含层的阀值调整量的计算更为复杂。假设是输入层第 $k$ 个节点和隐含层第 $i$ 个节点之间的权值，那么有</p>
<p>$$<br>\frac{\partial E\left( w,b \right)}{\partial w_{ki}}=\frac{1}{\partial w_{ki}}\cdot \frac{1}{2}\sum_{j=0}^{n-1}{\left( d_j-y_j \right) ^2}<br>$$<br>$$<br>=\sum_{j=0}^{n-1}{\left( d_j-y_j \right) \cdot f’\left( S_j \right) \cdot \frac{\partial S_j}{\partial w_{kj}}}<br>$$<br>$$<br>=\sum_{j=0}^{n-1}{\left( d_j-y_j \right) \cdot f’\left( S_j \right) \cdot \frac{\partial S_j}{\partial x_i}\cdot \frac{\partial x_i}{\partial S_i}\cdot \frac{\partial S_j}{\partial w_{kj}}}<br>$$<br>$$<br>=\sum_{j=0}^{n-1}{\delta_{ij}\cdot w_{ij}\cdot \frac{f\left( S_j \right) \left[ A-f\left( S_j \right) \right]}{AB}}\cdot x_k<br>$$<br>$$<br>=x_k\cdot \sum_{j=0}^{n-1}{\delta_{ij}\cdot w_{ij}\cdot \frac{f\left( S_j \right) \left[ A-f\left( S_j \right) \right]}{AB}}<br>$$<br>$$<br>=\delta_{ki}\cdot x_k<br>$$<br> 其中有<br> $$<br>\delta_{ki}=\sum_{j=0}^{n-1}{\delta_{ij}\cdot w_{ij}\cdot \frac{f\left( S_j \right) \left[ A-f\left( S_j \right) \right]}{AB}}<br>$$</p>
<h2 id="梯度下降更新"><a href="#梯度下降更新" class="headerlink" title="梯度下降更新"></a>梯度下降更新</h2><p>有了上述公式，根据梯度下降法，那么对于隐含层和输出层之间的权值和阀值调整如下<br>$$<br>w_{ij}=w_{ij}-\eta_1\frac{\partial E\left( w,b \right)}{\partial w_{ij}}=w_{ij}-\eta_1\delta_{ij}\cdot x_i<br>$$</p>
<p>$$<br>b_j=b_j-\eta_2\frac{\partial E\left( w,b \right)}{\partial b_j}=b_j-\eta_2\delta_{ij}<br>$$<br>  而对于输入层和隐含层之间的权值和阀值调整同样有</p>
<p>  $$w_{ki}=w_{ki}-\eta_1\frac{\partial E\left( w,b \right)}{\partial w_{ki}}=w_{ki}-\eta_1\delta_{ki}\cdot x_k$$</p>
<p>$$b_i=b_i-\eta_2\frac{\partial E\left( w,b \right)}{\partial b_i}=b_i-\eta_2\delta_{ki}$$</p>
<h2 id="bp的缺陷"><a href="#bp的缺陷" class="headerlink" title="bp的缺陷"></a>bp的缺陷</h2><ol>
<li>容易形成局部极小值而得不到全局最优值。BP神经网络中极小值比较多，所以很容易陷入局部极小值，这就要求对初始权值和阀值有要求，要使得初始权值和阀值随机性足够好，可以多次随机来实现。</li>
<li>训练次数多使得学习效率低，收敛速度慢。</li>
<li>隐含层的选取缺乏理论的指导。</li>
<li>训练时学习新样本有遗忘旧样本的趋势。<h2 id="卷积的后向传播"><a href="#卷积的后向传播" class="headerlink" title="卷积的后向传播"></a>卷积的后向传播</h2>在反向传播过程中，若第x层的a节点通过权值W对x+1层的b节点有贡献，则在反向传播过程中，梯度通过权值W从b节点传播回a节点。不管下面的公式推导，还是后面的卷积神经网络，在反向传播的过程中，都是遵循这样的一个规律。</li>
</ol>
<p>卷积层的反向传播过程也是如此，我们只需要找出卷积层L中的每个单元和L+1层中的哪些单元相关联即可。我们还用此的图片举例子。<br><img src="/images/cnn/7.png" alt=""><br>在上图中，我们的矩阵A11通过权重B11与C11关联。而A12与2个矩阵C中2个元素相关联，分别是通过权重B12和C11关联，和通过权重B11和C12相关联。矩阵A中其他元素也类似。</p>
<p>那么，我们有没有简单的方法来实现这样的关联呢。答案是有的。可以通过将卷积核旋转180度，再与扩充后的梯度矩阵进行卷积。扩充的过程如下：如果卷积核为 $k\times k$ ，待卷积矩阵为 $n\times n$ ，需要以$n\times n$ 原矩阵为中心扩展到 $(n+2(k-1))\times (n+2(k-1))$ 。具体过程如下：<br>假设D为反向传播到卷积层的梯度矩阵，则D应该与矩阵C的大小相等，在这里为3*3。我们首先需要将它扩充到 $(3+2\times(2-1))\times (3+2\times(2-1)) = 5\times 5$ 大小的矩阵，<br><img src="/images/cnn/9.png" alt=""><br>同时将卷积核B旋转180度：<br><img src="/images/cnn/10.png" alt=""><br>将旋转后的卷积核与扩充后的梯度矩阵进行卷积：<br><img src="/images/cnn/11.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好效果。然而，讨论将还是从对生物系统的一个高层次的简略描述开始，因为神经网络毕竟是从这里得到了启发.&lt;br&gt;
    
    </summary>
    
      <category term="卷积深度网络全剖析" scheme="http://yoursite.com/categories/%E5%8D%B7%E7%A7%AF%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E5%85%A8%E5%89%96%E6%9E%90/"/>
    
    
      <category term="卷积" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>使用hexo快速搭建github pages博客</title>
    <link href="http://yoursite.com/2016/11/01/%E4%BD%BF%E7%94%A8hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAgit%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2016/11/01/使用hexo快速搭建git博客/</id>
    <published>2016-11-01T15:40:13.000Z</published>
    <updated>2016-11-10T12:11:55.204Z</updated>
    
    <content type="html"><![CDATA[<h2 id="github-pages简介"><a href="#github-pages简介" class="headerlink" title=" github pages简介"></a> github pages简介</h2><p><img src="/images/gitblog/githubpages.png" alt=""></p>
<p>Github Pages 是 github 公司提供的免费的静态网站托管服务，用起来方便而且功能强大，不仅没有空间限制，还可以绑定自己的域名。在 <a href="https://pages.github.com/" target="_blank" rel="external">https://pages.github.com/</a> 首页上可以看到很多用 Github Pages 托管的网站，很漂亮。另外很多非常著名的公司和项目也都用这种方式来搭建网站，如微软和 twitter 的网站，还有 谷歌的 Material Design 图标 网站。<br><a id="more"></a></p>
<h2 id="node-js之hexo"><a href="#node-js之hexo" class="headerlink" title=" node.js之hexo"></a> node.js之hexo</h2><p>Node.js®是一个基于Chrome V8 引擎的 JavaScript 运行时。 Node.js 使用高效、轻量级的事件驱动、非阻塞 I/O 模型。Node.js 之生态系统是目前最大的开源包管理系统。</p>
<h2 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h2><p><img src="/images/gitblog/hexo.png" alt=""><br>Hexo 是高效的静态站点生成框架，她基于 Node.js。 通过 Hexo 你可以轻松地使用 Markdown 编写文章，除了 Markdown 本身的语法之外，还可以使用 Hexo 提供的 标签插件 来快速的插入特定形式的内容。在这篇文章中，假定你已经成功安装了 Hexo，并使用 Hexo 提供的命令创建了一个站点。</p>
<p>hexo出自台湾大学生tommy351之手，是一个基于Node.js的静态博客程序，其编译上百篇文字只需要几秒。hexo生成的静态网页可以直接放到GitHub Pages，BAE，SAE等平台上</p>
<h2 id="hexo之NexT主题"><a href="#hexo之NexT主题" class="headerlink" title=" hexo之NexT主题"></a> hexo之NexT主题</h2><p> 一个很好的基于hexo的主题，在NexT主题里有 scheme小主题的概念，非常棒，简约风格。<br> <img src="/images/gitblog/next.png" alt=""></p>
<h2 id="搭建博客系统"><a href="#搭建博客系统" class="headerlink" title=" 搭建博客系统"></a> 搭建博客系统</h2><p> 首先下载安装node.js <a href="https://nodejs.org/zh-cn/" target="_blank" rel="external">https://nodejs.org/zh-cn/</a></p>
<p>  下载之后打开node.js command prompt<br>  开始下载hexo<br>  运行如下的命令</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-cli -g</div><div class="line">$ hexo init blog</div><div class="line">$ cd blog</div><div class="line">$ npm install</div><div class="line">$ hexo server</div><div class="line"></div><div class="line">#安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。</div><div class="line"></div><div class="line">#新建完成后，指定文件夹的目录如下</div><div class="line">.</div><div class="line">├── _config.yml</div><div class="line">├── package.json</div><div class="line">├── scaffolds</div><div class="line">├── scripts</div><div class="line">├── source</div><div class="line">|      ├── _drafts</div><div class="line">|      └── _posts</div><div class="line">└── themes</div></pre></td></tr></table></figure>
<p>  这时候使用<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo s</div></pre></td></tr></table></figure></p>
<p>  就可以打开 localhost: 4000进行访问看到hexo的landspace 就成功了</p>
<p>note：<br><code>如果你的安装了福新阅读器等一些占用4000端口的程序。就会加载不出来。</code></p>
<p>可以更改端口：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo s -p 3600</div></pre></td></tr></table></figure>
<p>安装成功之后可以绑定自己的gitHub.<br>后面的NexT主题安装部分详细见：</p>
<p><a href="http://theme-next.iissnan.com/" target="_blank" rel="external">http://theme-next.iissnan.com/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;github-pages简介&quot;&gt;&lt;a href=&quot;#github-pages简介&quot; class=&quot;headerlink&quot; title=&quot; github pages简介&quot;&gt;&lt;/a&gt; github pages简介&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/gitblog/githubpages.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Github Pages 是 github 公司提供的免费的静态网站托管服务，用起来方便而且功能强大，不仅没有空间限制，还可以绑定自己的域名。在 &lt;a href=&quot;https://pages.github.com/&quot;&gt;https://pages.github.com/&lt;/a&gt; 首页上可以看到很多用 Github Pages 托管的网站，很漂亮。另外很多非常著名的公司和项目也都用这种方式来搭建网站，如微软和 twitter 的网站，还有 谷歌的 Material Design 图标 网站。&lt;br&gt;
    
    </summary>
    
      <category term="独立博客搭建" scheme="http://yoursite.com/categories/%E7%8B%AC%E7%AB%8B%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
</feed>
